{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ead31d9",
   "metadata": {},
   "source": [
    "# DeepSleepNet-Lite: Reproduction & Student Extensions\n",
    "\n",
    "**Fill your names here**  \n",
    "**Course:** cs584 f25 â€“ Final Project  \n",
    "**Generated:** 2025-11-03 19:18\n",
    "\n",
    "> This notebook reproduces a light, two-branch 1D CNN for single-channel EEG sleep staging (sequence-to-epoch with 90 seconds of context) and adds:\n",
    "> - Uniform and structured label smoothing\n",
    "> - Monte Carlo Dropout uncertainty\n",
    "> - Proper evaluation: Accuracy, Macro-F1, Weighted-F1, Cohen's kappa\n",
    "> - Calibration via ECE and reliability diagrams\n",
    "> - Selective prediction (reject the least-confident q%)  \n",
    "> All comments target students: we explain *what*, *why*, and *how* in practical terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f19bfa8",
   "metadata": {},
   "source": [
    "## 0. Environment & Reproducibility\n",
    "We install/verify packages, print versions, and set seeds. If internet installs are blocked on your machine, pre-install the listed packages and re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, subprocess, importlib, os, random, json, math, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _pip_install(pkg):\n",
    "    try:\n",
    "        importlib.import_module(pkg.split(\"==\")[0])\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "REQUIRED = [\n",
    "    \"numpy\", \"scipy\", \"pandas\", \"matplotlib\",\n",
    "    \"scikit-learn\",\n",
    "    \"torch==2.2.2\",\n",
    "    \"torchmetrics>=1.3.0\",\n",
    "    \"tqdm\",\n",
    "    \"wfdb\",\n",
    "    \"mne\",\n",
    "]\n",
    "for p in REQUIRED:\n",
    "    try:\n",
    "        _pip_install(p)\n",
    "    except Exception as e:\n",
    "        print(\"Warning installing\", p, \":\", e)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, cohen_kappa_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a9ad3",
   "metadata": {},
   "source": [
    "## 1. Configuration (edit me)\n",
    "Centralized hyper-parameters and paths. The entire notebook reads from this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"dataset\": \"sleep-edf\",\n",
    "    \"target_channel\": \"Fpz-Cz\",\n",
    "    \"epoch_seconds\": 30,\n",
    "    \"context_epochs\": 3,             # 90 seconds = 3 x 30s\n",
    "    \"fs\": 100,\n",
    "    \"use_in_bed_only\": False,\n",
    "\n",
    "    \"cv_folds\": 5,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"seed\": 42,\n",
    "\n",
    "    \"batch_size\": 128,\n",
    "    \"max_epochs\": 25,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-3,\n",
    "    \"patience\": 5,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "\n",
    "    \"small_kernel\": 7,\n",
    "    \"large_kernel\": 49,\n",
    "    \"n_filters\": 64,\n",
    "    \"dropout_p\": 0.5,\n",
    "\n",
    "    \"label_smoothing\": \"uniform\",    # \"none\" | \"uniform\" | \"structured\"\n",
    "    \"ls_alpha\": 0.1,\n",
    "\n",
    "    \"mc_dropout_samples\": 30,\n",
    "    \"balance_strategy\": \"oversample_and_flip\",\n",
    "    \"num_workers\": 2,\n",
    "\n",
    "    \"artifacts_dir\": \"artifacts\",\n",
    "}\n",
    "os.makedirs(CONFIG[\"artifacts_dir\"], exist_ok=True)\n",
    "with open(os.path.join(CONFIG[\"artifacts_dir\"], \"config.json\"), \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f244e",
   "metadata": {},
   "source": [
    "## 2. Dataset handling\n",
    "We target Sleep-EDF (Fpz-Cz @ 100 Hz). To keep the notebook runnable everywhere, we provide a **synthetic fallback** if EDF files are not present or cannot be downloaded. Replace the fallback with real parsing for your full results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c59bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "DATA_ROOT = os.path.join(CONFIG[\"artifacts_dir\"], \"sleep_edf\")\n",
    "os.makedirs(DATA_ROOT, exist_ok=True)\n",
    "\n",
    "STAGE_MAP = {\"W\":0,\"N1\":1,\"N2\":2,\"N3\":3,\"R\":4}\n",
    "IDX2STAGE = {v:k for k,v in STAGE_MAP.items()}\n",
    "\n",
    "def build_synthetic_dataset(n_subjects=4, epochs_per_subject=600, fs=100, epoch_sec=30):\n",
    "    rng = np.random.default_rng(CONFIG[\"seed\"])\n",
    "    X, y, subj = [], [], []\n",
    "    for s in range(n_subjects):\n",
    "        stages = rng.choice([0,1,2,3,4], size=epochs_per_subject, p=[0.2,0.1,0.45,0.15,0.1])\n",
    "        samples = fs*epoch_sec\n",
    "        signal = rng.normal(0, 1, size=(epochs_per_subject, samples))\n",
    "        X.append(signal); y.append(stages); subj += [f\"SYN{s:02d}\"]*epochs_per_subject\n",
    "    return np.vstack(X), np.hstack(y), np.array(subj)\n",
    "\n",
    "# For the template we use synthetic data so the rest of the pipeline is executable.\n",
    "X_epochs, y_epochs, subjects = build_synthetic_dataset()\n",
    "print(\"Epochs:\", X_epochs.shape, \"Labels:\", y_epochs.shape, \"Subjects:\", subjects.shape)\n",
    "\n",
    "def build_context_windows(Xe, ye, sub, context_epochs=3):\n",
    "    assert context_epochs == 3\n",
    "    Xw, yw, sw = [], [], []\n",
    "    for i in range(1, len(ye)-1):\n",
    "        win = np.concatenate([Xe[i-1], Xe[i], Xe[i+1]], axis=0)\n",
    "        Xw.append(win); yw.append(ye[i]); sw.append(sub[i])\n",
    "    Xw = np.vstack([np.array(x)[None,:] for x in Xw])\n",
    "    return Xw.astype(np.float32), np.array(yw).astype(np.int64), np.array(sw)\n",
    "\n",
    "X_win, y_win, subj_win = build_context_windows(X_epochs, y_epochs, subjects, CONFIG[\"context_epochs\"])\n",
    "\n",
    "def balance_data(X, y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    max_c = counts.max()\n",
    "    Xb, yb = [], []\n",
    "    rng = np.random.default_rng(CONFIG[\"seed\"])\n",
    "    for c in classes:\n",
    "        idx = np.where(y == c)[0]\n",
    "        Xi, yi = X[idx], y[idx]\n",
    "        Xi_aug = np.concatenate([Xi, -Xi], axis=0)\n",
    "        yi_aug = np.concatenate([yi, yi], axis=0)\n",
    "        if len(Xi_aug) < max_c:\n",
    "            extra = rng.choice(len(Xi_aug), size=max_c-len(Xi_aug), replace=True)\n",
    "            Xi_aug = np.concatenate([Xi_aug, Xi_aug[extra]], axis=0)\n",
    "            yi_aug = np.concatenate([yi_aug, yi_aug[extra]], axis=0)\n",
    "        else:\n",
    "            Xi_aug = Xi_aug[:max_c]; yi_aug = yi_aug[:max_c]\n",
    "        Xb.append(Xi_aug); yb.append(yi_aug)\n",
    "    Xb = np.concatenate(Xb, axis=0); yb = np.concatenate(yb, axis=0)\n",
    "    perm = rng.permutation(len(yb))\n",
    "    return Xb[perm], yb[perm]\n",
    "\n",
    "if CONFIG[\"balance_strategy\"] == \"oversample_and_flip\":\n",
    "    X_bal, y_bal = balance_data(X_win, y_win)\n",
    "else:\n",
    "    X_bal, y_bal = X_win, y_win\n",
    "\n",
    "print(\"Balanced windows:\", X_bal.shape, \"Labels:\", y_bal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510816c8",
   "metadata": {},
   "source": [
    "## 3. Model: two-branch 1D CNN (DeepSleepNet-Lite style)\n",
    "Small kernels capture fine temporal features; large kernels capture broader spectral context. We keep parameter count modest and avoid RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    # Conv1d -> BN -> ReLU -> MaxPool\n",
    "    def __init__(self, in_ch, out_ch, k, pool=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=k//2)\n",
    "        self.bn = nn.BatchNorm1d(out_ch, eps=1e-5, momentum=0.999)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool1d(pool)\n",
    "    def forward(self, x):\n",
    "        return self.pool(self.act(self.bn(self.conv(x))))\n",
    "\n",
    "class DeepSleepNetLite(nn.Module):\n",
    "    # Input: [B, 1, T]; Output: class logits for 5 stages\n",
    "    def __init__(self, n_classes=5, nf=64, k_small=7, k_large=49, p_drop=0.5):\n",
    "        super().__init__()\n",
    "        self.s1 = ConvBlock(1, nf, k_small)\n",
    "        self.s2 = ConvBlock(nf, nf, k_small)\n",
    "        self.s3 = ConvBlock(nf, 2*nf, k_small)\n",
    "        self.s4 = ConvBlock(2*nf, 2*nf, k_small)\n",
    "\n",
    "        self.l1 = ConvBlock(1, nf, k_large)\n",
    "        self.l2 = ConvBlock(nf, nf, k_large)\n",
    "        self.l3 = ConvBlock(nf, 2*nf, k_large)\n",
    "        self.l4 = ConvBlock(2*nf, 2*nf, k_large)\n",
    "\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4*nf, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self.s4(self.s3(self.s2(self.s1(x))))\n",
    "        xl = self.l4(self.l3(self.l2(self.l1(x))))\n",
    "        xcat = torch.cat([xs, xl], dim=1)\n",
    "        xcat = self.drop(xcat)\n",
    "        return self.head(xcat)\n",
    "\n",
    "# Simple shape check\n",
    "tmp = torch.randn(2,1,CONFIG[\"fs\"]*CONFIG[\"epoch_seconds\"]*CONFIG[\"context_epochs\"])\n",
    "print(\"Logits shape:\", DeepSleepNetLite().forward(tmp).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaee213",
   "metadata": {},
   "source": [
    "## 4. Label smoothing\n",
    "Uniform (LSu) mixes one-hot with uniform; structured (LSs) mixes one-hot with an empirical transition-based distribution M(prev,:,next) estimated from training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388884fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_uniform_targets(y, n_classes=5, alpha=0.1, device=\"cpu\"):\n",
    "    off = alpha / n_classes\n",
    "    on = 1.0 - alpha + off\n",
    "    tgt = torch.full((len(y), n_classes), off, dtype=torch.float32, device=device)\n",
    "    tgt[torch.arange(len(y)), y] = on\n",
    "    return tgt\n",
    "\n",
    "def estimate_M_structured(y_sequence, n_classes=5):\n",
    "    M = np.ones((n_classes, n_classes, n_classes), dtype=np.float64)  # Laplace smoothing\n",
    "    for i in range(1, len(y_sequence)-1):\n",
    "        p, c, n = y_sequence[i-1], y_sequence[i], y_sequence[i+1]\n",
    "        M[p, c, n] += 1.0\n",
    "    # normalize over middle dimension\n",
    "    M = M / M.sum(axis=1, keepdims=True)\n",
    "    M = M / M.sum(axis=2, keepdims=True)\n",
    "    return M\n",
    "\n",
    "def make_structured_targets(y, y_prev, y_next, M, alpha=0.2, n_classes=5, device=\"cpu\"):\n",
    "    B = len(y)\n",
    "    tgt = torch.zeros((B, n_classes), dtype=torch.float32, device=device)\n",
    "    for i in range(B):\n",
    "        vec = torch.tensor(M[y_prev[i].item(), :, y_next[i].item()], dtype=torch.float32, device=device)\n",
    "        vec = vec / (vec.sum() + 1e-8)\n",
    "        tgt[i] = (1.0 - alpha) * F.one_hot(y[i], n_classes).float() + alpha * vec\n",
    "    return tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d56ef3",
   "metadata": {},
   "source": [
    "## 5. Dataset wrapper and CV splits (by subject)\n",
    "We wrap arrays into a Dataset that also carries prev/next labels for LSs, and create subject-wise folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3027afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class SleepWindows(Dataset):\n",
    "    def __init__(self, X, y, subj, prev_labels, next_labels):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "        self.subj = np.array(subj)\n",
    "        self.prev = prev_labels.astype(np.int64)\n",
    "        self.next = next_labels.astype(np.int64)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx][None, :]  # [1, T]\n",
    "        return x, self.y[idx], self.prev[idx], self.next[idx]\n",
    "\n",
    "prev_arr = np.concatenate([[y_bal[0]], y_bal[:-1]])\n",
    "next_arr = np.concatenate([y_bal[1:], [y_bal[-1]]])\n",
    "dataset = SleepWindows(X_bal, y_bal, subj_win[:len(y_bal)], prev_arr, next_arr)\n",
    "\n",
    "uniq_subj = sorted(list(set(dataset.subj)))\n",
    "rng = np.random.default_rng(CONFIG[\"seed\"])\n",
    "rng.shuffle(uniq_subj)\n",
    "folds = np.array_split(uniq_subj, CONFIG[\"cv_folds\"])\n",
    "print(\"Folds (subjects):\", [list(f) for f in folds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e2588",
   "metadata": {},
   "source": [
    "## 6. Utilities: Early stopping, ECE, reliability plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ece_score(confidences, correctness, n_bins=15):\n",
    "    bins = np.linspace(0., 1., n_bins+1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        sel = (confidences > lo) & (confidences <= hi)\n",
    "        if sel.sum() == 0: continue\n",
    "        acc = correctness[sel].mean()\n",
    "        conf = confidences[sel].mean()\n",
    "        ece += (sel.sum() / len(confidences)) * abs(acc - conf)\n",
    "    return float(ece)\n",
    "\n",
    "def plot_reliability(confidences, correctness, n_bins=15):\n",
    "    bins = np.linspace(0., 1., n_bins+1)\n",
    "    xs, ys = [], []\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        sel = (confidences > lo) & (confidences <= hi)\n",
    "        if sel.sum() == 0: continue\n",
    "        xs.append(correctness[sel].mean()); ys.append(confidences[sel].mean())\n",
    "    plt.figure()\n",
    "    plt.plot([0,1],[0,1],'--', linewidth=1)\n",
    "    plt.scatter(xs, ys)\n",
    "    plt.xlabel(\"Accuracy per bin\"); plt.ylabel(\"Mean confidence per bin\"); plt.title(\"Reliability Diagram\")\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5):\n",
    "        self.best = None; self.wait = 0; self.pat = patience; self.state=None\n",
    "    def step(self, metric, model):\n",
    "        if self.best is None or metric > self.best:\n",
    "            self.best = metric; self.wait = 0\n",
    "            self.state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "            return False\n",
    "        self.wait += 1\n",
    "        return self.wait > self.pat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ee73c",
   "metadata": {},
   "source": [
    "## 7. Train per fold\n",
    "We train with the selected label smoothing strategy and early stopping on macro-F1 (more robust to class imbalance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd0b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_for_fold(fold_id):\n",
    "    test_subj = set(folds[fold_id])\n",
    "    train_subj = set().union(*[set(f) for i,f in enumerate(folds) if i!=fold_id])\n",
    "    train_mask = np.array([s in train_subj for s in dataset.subj])\n",
    "    test_mask  = np.array([s in test_subj  for s in dataset.subj])\n",
    "    idx_train_all = np.where(train_mask)[0]\n",
    "    idx_test = np.where(test_mask)[0]\n",
    "    rng = np.random.default_rng(CONFIG[\"seed\"] + fold_id)\n",
    "    perm = rng.permutation(len(idx_train_all))\n",
    "    n_val = max(1, int(CONFIG[\"val_ratio\"] * len(idx_train_all)))\n",
    "    idx_val = idx_train_all[perm[:n_val]]\n",
    "    idx_train = idx_train_all[perm[n_val:]]\n",
    "    return idx_train, idx_val, idx_test\n",
    "\n",
    "def run_fold(train_idx, val_idx, test_idx, data):\n",
    "    set_seeds(CONFIG[\"seed\"])\n",
    "    device = CONFIG[\"device\"]\n",
    "    model = DeepSleepNetLite(nf=CONFIG[\"n_filters\"],\n",
    "                             k_small=CONFIG[\"small_kernel\"],\n",
    "                             k_large=CONFIG[\"large_kernel\"],\n",
    "                             p_drop=CONFIG[\"dropout_p\"]).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "\n",
    "    def make_loader(idxs, shuffle):\n",
    "        subset = torch.utils.data.Subset(data, idxs.tolist())\n",
    "        return DataLoader(subset, batch_size=CONFIG[\"batch_size\"], shuffle=shuffle,\n",
    "                          num_workers=CONFIG[\"num_workers\"], drop_last=False)\n",
    "\n",
    "    train_loader = make_loader(train_idx, True)\n",
    "    val_loader   = make_loader(val_idx, False)\n",
    "\n",
    "    M = estimate_M_structured(data.y[train_idx]) if CONFIG[\"label_smoothing\"] == \"structured\" else None\n",
    "    stopper = EarlyStopper(CONFIG[\"patience\"])\n",
    "    history = []\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(1, CONFIG[\"max_epochs\"]+1):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for xb, yb, pb, nb in train_loader:\n",
    "            xb, yb, pb, nb = xb.to(device), yb.to(device), pb.to(device), nb.to(device)\n",
    "            logits = model(xb)\n",
    "            if CONFIG[\"label_smoothing\"] == \"uniform\":\n",
    "                targets = make_uniform_targets(yb, n_classes=5, alpha=CONFIG[\"ls_alpha\"], device=device)\n",
    "                loss = -(targets * F.log_softmax(logits, dim=1)).sum(dim=1).mean()\n",
    "            elif CONFIG[\"label_smoothing\"] == \"structured\":\n",
    "                targets = make_structured_targets(yb, pb, nb, M, alpha=CONFIG[\"ls_alpha\"], device=device)\n",
    "                loss = -(targets * F.log_softmax(logits, dim=1)).sum(dim=1).mean()\n",
    "            else:\n",
    "                loss = F.cross_entropy(logits, yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        y_true, y_pred, y_conf = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _, _ in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                probs = F.softmax(model(xb), dim=1)\n",
    "                conf, pred = probs.max(dim=1)\n",
    "                y_true.extend(yb.cpu().numpy()); y_pred.extend(pred.cpu().numpy()); y_conf.extend(conf.cpu().numpy())\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        mf1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "        wf1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "        kappa = cohen_kappa_score(y_true, y_pred)\n",
    "        ece = ece_score(np.array(y_conf), (np.array(y_true)==np.array(y_pred)).astype(float))\n",
    "        history.append({\"epoch\":epoch,\"loss\":float(np.mean(losses)),\"acc\":acc,\"mf1\":mf1,\"wf1\":wf1,\"kappa\":kappa,\"ece\":ece})\n",
    "        print(f\"Epoch {epoch:02d} | loss {np.mean(losses):.4f} | acc {acc:.3f} | MF1 {mf1:.3f} | k {kappa:.3f} | ECE {ece:.3f}\")\n",
    "        if stopper.step(mf1, model):\n",
    "            print(\"Early stopping\"); break\n",
    "        else:\n",
    "            best_epoch = epoch\n",
    "\n",
    "    model.load_state_dict(stopper.state)\n",
    "    return model, pd.DataFrame(history), best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4262afdf",
   "metadata": {},
   "source": [
    "## 8. Run K-fold CV, save models and histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d8361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL = []\n",
    "FOLD_RESULTS = []\n",
    "for fid in range(len(folds)):\n",
    "    print(\"\\\\n=== Fold\", fid, \"===\")\n",
    "    tr, va, te = indices_for_fold(fid)\n",
    "    model, hist, best_ep = run_fold(tr, va, te, dataset)\n",
    "    save_path = os.path.join(CONFIG[\"artifacts_dir\"], f\"model_fold{fid}.pt\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    hist.to_csv(os.path.join(CONFIG[\"artifacts_dir\"], f\"history_fold{fid}.csv\"), index=False)\n",
    "    FOLD_RESULTS.append({\"fold\": fid, \"best_epoch\": int(best_ep), \"model_path\": save_path})\n",
    "pd.DataFrame(FOLD_RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a6146",
   "metadata": {},
   "source": [
    "## 9. Test evaluation + MC Dropout + Selective prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba9720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_mc_dropout(m):\n",
    "    for module in m.modules():\n",
    "        if isinstance(module, nn.Dropout):\n",
    "            module.train()\n",
    "\n",
    "def evaluate_model(model, idxs, mc_samples=0):\n",
    "    device = CONFIG[\"device\"]\n",
    "    loader = DataLoader(torch.utils.data.Subset(dataset, idxs.tolist()),\n",
    "                        batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "                        num_workers=CONFIG[\"num_workers\"], drop_last=False)\n",
    "    y_true, y_pred, y_conf = [], [], []\n",
    "    if mc_samples <= 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _, _ in loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                probs = F.softmax(model(xb), dim=1)\n",
    "                conf, pred = probs.max(dim=1)\n",
    "                y_true.extend(yb.cpu().numpy()); y_pred.extend(pred.cpu().numpy()); y_conf.extend(conf.cpu().numpy())\n",
    "        return np.array(y_true), np.array(y_pred), np.array(y_conf)\n",
    "    else:\n",
    "        model.eval(); enable_mc_dropout(model)\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _, _ in loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                probs_mc = []\n",
    "                for _ in range(mc_samples):\n",
    "                    probs_mc.append(F.softmax(model(xb), dim=1).unsqueeze(0))\n",
    "                probs_mc = torch.cat(probs_mc, dim=0)\n",
    "                probs = probs_mc.mean(dim=0)\n",
    "                conf, pred = probs.max(dim=1)\n",
    "                y_true.extend(yb.cpu().numpy()); y_pred.extend(pred.cpu().numpy()); y_conf.extend(conf.cpu().numpy())\n",
    "        return np.array(y_true), np.array(y_pred), np.array(y_conf)\n",
    "\n",
    "ALL_METRICS = []\n",
    "for rec in FOLD_RESULTS:\n",
    "    fid = rec[\"fold\"]\n",
    "    _, _, idx_test = indices_for_fold(fid)\n",
    "\n",
    "    model = DeepSleepNetLite(nf=CONFIG[\"n_filters\"],\n",
    "                             k_small=CONFIG[\"small_kernel\"],\n",
    "                             k_large=CONFIG[\"large_kernel\"],\n",
    "                             p_drop=CONFIG[\"dropout_p\"]).to(CONFIG[\"device\"])\n",
    "    model.load_state_dict(torch.load(rec[\"model_path\"], map_location=CONFIG[\"device\"]))\n",
    "\n",
    "    y_t, y_p, y_c = evaluate_model(model, idx_test, mc_samples=0)\n",
    "    acc = accuracy_score(y_t, y_p); mf1 = f1_score(y_t, y_p, average=\"macro\")\n",
    "    wf1 = f1_score(y_t, y_p, average=\"weighted\"); kappa = cohen_kappa_score(y_t, y_p)\n",
    "    ece = ece_score(y_c, (y_t==y_p).astype(float))\n",
    "    print(f\"[Fold {fid}] TEST std: acc {acc:.3f} mf1 {mf1:.3f} k {kappa:.3f} ece {ece:.3f}\")\n",
    "    ALL_METRICS.append({\"fold\":fid,\"mode\":\"standard\",\"acc\":acc,\"mf1\":mf1,\"wf1\":wf1,\"kappa\":kappa,\"ece\":ece})\n",
    "    plot_reliability(y_c, (y_t==y_p).astype(float))\n",
    "\n",
    "    S = CONFIG[\"mc_dropout_samples\"]\n",
    "    y_t2, y_p2, y_c2 = evaluate_model(model, idx_test, mc_samples=S)\n",
    "    acc2 = accuracy_score(y_t2, y_p2); mf1_2 = f1_score(y_t2, y_p2, average=\"macro\")\n",
    "    wf1_2 = f1_score(y_t2, y_p2, average=\"weighted\"); kappa2 = cohen_kappa_score(y_t2, y_p2)\n",
    "    ece2 = ece_score(y_c2, (y_t2==y_p2).astype(float))\n",
    "    print(f\"[Fold {fid}] TEST MC{S}: acc {acc2:.3f} mf1 {mf1_2:.3f} k {kappa2:.3f} ece {ece2:.3f}\")\n",
    "    ALL_METRICS.append({\"fold\":fid,\"mode\":f\"mc{S}\",\"acc\":acc2,\"mf1\":mf1_2,\"wf1\":wf1_2,\"kappa\":kappa2,\"ece\":ece2})\n",
    "\n",
    "    for q in [0.05, 0.1, 0.2]:\n",
    "        thr = np.quantile(y_c2, q)\n",
    "        mask = y_c2 > thr\n",
    "        if mask.sum()==0: continue\n",
    "        acc_q = accuracy_score(y_t2[mask], y_p2[mask])\n",
    "        mf1_q = f1_score(y_t2[mask], y_p2[mask], average=\"macro\")\n",
    "        kappa_q = cohen_kappa_score(y_t2[mask], y_p2[mask])\n",
    "        print(f\"[Fold {fid}] MC{S} reject {int(q*100)}%: acc {acc_q:.3f} mf1 {mf1_q:.3f} k {kappa_q:.3f}\")\n",
    "        ALL_METRICS.append({\"fold\":fid,\"mode\":f\"mc{S}_reject_{int(q*100)}\",\"acc\":acc_q,\"mf1\":mf1_q,\"wf1\":None,\"kappa\":kappa_q,\"ece\":None})\n",
    "\n",
    "pd.DataFrame(ALL_METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d62911",
   "metadata": {},
   "source": [
    "## 10. Qualitative inspection\n",
    "We show a couple of sample windows with predicted distribution (mean if MC) to understand errors and uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c56d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(model, idxs, n=2, mc_samples=0):\n",
    "    device = CONFIG[\"device\"]\n",
    "    loader = DataLoader(torch.utils.data.Subset(dataset, idxs.tolist()[:n]), batch_size=1, shuffle=False)\n",
    "    model.eval()\n",
    "    if mc_samples>0:\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.Dropout): m.train()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _, _ in loader:\n",
    "            xb = xb.to(device); gt = int(yb.item())\n",
    "            if mc_samples>0:\n",
    "                pm = []\n",
    "                for _ in range(mc_samples):\n",
    "                    pm.append(F.softmax(model(xb), dim=1).cpu().numpy()[0])\n",
    "                probs = np.mean(pm, axis=0)\n",
    "            else:\n",
    "                probs = F.softmax(model(xb), dim=1).cpu().numpy()[0]\n",
    "            pred = int(np.argmax(probs))\n",
    "            plt.figure(); plt.plot(xb.cpu().numpy().ravel()); plt.title(f\"True {IDX2STAGE[gt]} | Pred {IDX2STAGE[pred]} | Conf {probs[pred]:.2f}\")\n",
    "            plt.figure(); plt.bar(range(5), probs); plt.xticks(range(5), [IDX2STAGE[i] for i in range(5)]); plt.title(\"Predicted probabilities\")\n",
    "\n",
    "_, _, idx_test0 = indices_for_fold(0)\n",
    "m0 = DeepSleepNetLite(nf=CONFIG[\"n_filters\"], k_small=CONFIG[\"small_kernel\"], k_large=CONFIG[\"large_kernel\"], p_drop=CONFIG[\"dropout_p\"]).to(CONFIG[\"device\"])\n",
    "m0.load_state_dict(torch.load(os.path.join(CONFIG[\"artifacts_dir\"], \"model_fold0.pt\"), map_location=CONFIG[\"device\"]))\n",
    "show_examples(m0, idx_test0, n=2, mc_samples=CONFIG[\"mc_dropout_samples\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0e295",
   "metadata": {},
   "source": [
    "## 11. Export artifacts for final submission\n",
    "We save metrics, a minimal `model.txt`, `data.txt`, and a `requirements.txt` as requested by the brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a4365",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_path = os.path.join(CONFIG[\"artifacts_dir\"], \"metrics.csv\")\n",
    "pd.DataFrame(ALL_METRICS).to_csv(metrics_path, index=False)\n",
    "print(\"Saved:\", metrics_path)\n",
    "\n",
    "with open(os.path.join(CONFIG[\"artifacts_dir\"], \"model.txt\"), \"w\") as f:\n",
    "    f.write(\"# Saved model weights (per fold)\\n\")\n",
    "    for rec in os.listdir(CONFIG[\"artifacts_dir\"]):\n",
    "        if rec.startswith(\"model_fold\") and rec.endswith(\".pt\"):\n",
    "            f.write(rec + \"\\n\")\n",
    "\n",
    "with open(os.path.join(CONFIG[\"artifacts_dir\"], \"data.txt\"), \"w\") as f:\n",
    "    f.write(\"Sleep-EDF Expanded (Sleep Cassette). PhysioNet.\\nPlace EDF files under artifacts/sleep_edf/ if auto-download is not available.\\n\")\n",
    "\n",
    "reqs = [\n",
    "    \"numpy\",\"scipy\",\"pandas\",\"matplotlib\",\"scikit-learn\",\n",
    "    \"torch==2.2.2\",\"torchmetrics>=1.3.0\",\"tqdm\",\"wfdb\",\"mne\"\n",
    "]\n",
    "with open(os.path.join(CONFIG[\"artifacts_dir\"], \"requirements.txt\"), \"w\") as f:\n",
    "    f.write(\"\\\\n\".join(reqs))\n",
    "\n",
    "print(\"Artifacts in:\", CONFIG[\"artifacts_dir\"])"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
