
"""
build_deepsleepnetlite_notebook.py
----------------------------------
This script generates a Jupyter notebook named "DeepSleepNetLite_Project_Notebook.ipynb"
that implements a student-focused reproduction of DeepSleepNet-Lite with:
 - Two-branch 1D CNN
 - Uniform and structured label smoothing
 - Monte Carlo Dropout uncertainty
 - Accuracy, Macro-F1, Weighted-F1, Cohen's kappa
 - ECE + reliability diagram
 - Selective prediction
All comments in the notebook are in English.
"""

import os, json
from datetime import datetime
import nbformat
from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell

def md(s): return new_markdown_cell(s.strip())
def code(s): return new_code_cell(s.strip())

def build():
    nb = new_notebook()
    cells = []

    def add_md(s): cells.append(md(s))
    def add_code(s): cells.append(code(s))

    add_md(f"# DeepSleepNet-Lite: Reproduction & Student Extensions\\n\\nGenerated: {datetime.now():%Y-%m-%d %H:%M}")
    add_md("## 0. Environment & Reproducibility")
    add_code(\"\"\"
# (Same content as in the inline notebook inside ChatGPT.)
# To keep this script compact, we embed the exact same cells here.
# For brevity in this generator file, we import the text of the cells from a small payload below.
from textwrap import dedent

payload = NOTEBOOK_PAYLOAD.strip().split(\"\\n\\n### CELL SPLIT ###\\n\\n\")
for chunk in payload:
    kind, body = chunk.split(\"\\n\", 1)
    if kind.strip() == \"#MD#\":
        get_ipython().set_next_input(dedent(body).strip(), replace=True)
    else:
        print(dedent(body))  # placeholder; Jupyter will replace when executed
\"\"\" )

    # Instead of reconstructing every cell again, we store the full notebook
    # source in a literal (NOTEBOOK_FULL) and write it directly.
    NOTEBOOK_FULL = '{\n "cells": [\n  {\n   "cell_type": "markdown",\n   "id": "9ead31d9",\n   "metadata": {},\n   "source": [\n    "# DeepSleepNet-Lite: Reproduction & Student Extensions\\n",\n    "\\n",\n    "**Fill your names here**  \\n",\n    "**Course:** cs584 f25 â€“ Final Project  \\n",\n    "**Generated:** 2025-11-03 19:18\\n",\n    "\\n",\n    "> This notebook reproduces a light, two-branch 1D CNN for single-channel EEG sleep staging (sequence-to-epoch with 90 seconds of context) and adds:\\n",\n    "> - Uniform and structured label smoothing\\n",\n    "> - Monte Carlo Dropout uncertainty\\n",\n    "> - Proper evaluation: Accuracy, Macro-F1, Weighted-F1, Cohen\'s kappa\\n",\n    "> - Calibration via ECE and reliability diagrams\\n",\n    "> - Selective prediction (reject the least-confident q%)  \\n",\n    "> All comments target students: we explain *what*, *why*, and *how* in practical terms."\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "id": "2f19bfa8",\n   "metadata": {},\n   "source": [\n    "## 0. Environment & Reproducibility\\n",\n    "We install/verify packages, print versions, and set seeds. If internet installs are blocked on your machine, pre-install the listed packages and re-run."\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "id": "2171761c",\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "import sys, subprocess, importlib, os, random, json, math, time\\n",\n    "import numpy as np\\n",\n    "import pandas as pd\\n",\n    "\\n",\n    "def _pip_install(pkg):\\n",\n    "    try:\\n",\n    "        importlib.import_module(pkg.split(\\"==\\")[0])\\n",\n    "    except Exception:\\n",\n    "        subprocess.check_call([sys.executable, \\"-m\\", \\"pip\\", \\"install\\", pkg])\\n",\n    "\\n",\n    "REQUIRED = [\\n",\n    "    \\"numpy\\", \\"scipy\\", \\"pandas\\", \\"matplotlib\\",\\n",\n    "    \\"scikit-learn\\",\\n",\n    "    \\"torch==2.2.2\\",\\n",\n    "    \\"torchmetrics>=1.3.0\\",\\n",\n    "    \\"tqdm\\",\\n",\n    "    \\"wfdb\\",\\n",\n    "    \\"mne\\",\\n",\n    "]\\n",\n    "for p in REQUIRED:\\n",\n    "    try:\\n",\n    "        _pip_install(p)\\n",\n    "    except Exception as e:\\n",\n    "        print(\\"Warning installing\\", p, \\":\\", e)\\n",\n    "\\n",\n    "import matplotlib\\n",\n    "import matplotlib.pyplot as plt\\n",\n    "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\\n",\n    "from torch.utils.data import Dataset, DataLoader\\n",\n    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, cohen_kappa_score\\n",\n    "from tqdm import tqdm\\n",\n    "\\n",\n    "print(\\"Python:\\", sys.version)\\n",\n    "print(\\"numpy:\\", np.__version__)\\n",\n    "print(\\"pandas:\\", pd.__version__)\\n",\n    "print(\\"matplotlib:\\", matplotlib.__version__)\\n",\n    "print(\\"torch:\\", torch.__version__)\\n",\n    "\\n",\n    "def set_seeds(seed=42):\\n",\n    "    random.seed(seed); np.random.seed(seed)\\n",\n    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\\n",\n    "    torch.backends.cudnn.deterministic = True\\n",\n    "    torch.backends.cudnn.benchmark = False"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "id": "7c5a9ad3",\n   "metadata": {},\n   "source": [\n    "## 1. Configuration (edit me)\\n",\n    "Centralized hyper-parameters and paths. The entire notebook reads from this dictionary."\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "id": "0b7d1a8b",\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "CONFIG = {\\n",\n    "    \\"dataset\\": \\"sleep-edf\\",\\n",\n    "    \\"target_channel\\": \\"Fpz-Cz\\",\\n",\n    "    \\"epoch_seconds\\": 30,\\n",\n    "    \\"context_epochs\\": 3,             # 90 seconds = 3 x 30s\\n",\n    "    \\"fs\\": 100,\\n",\n    "    \\"use_in_bed_only\\": False,\\n",\n    "\\n",\n    "    \\"cv_folds\\": 5,\\n",\n    "    \\"val_ratio\\": 0.1,\\n",\n    "    \\"seed\\": 42,\\n",\n    "\\n",\n    "    \\"batch_size\\": 128,\\n",\n    "    \\"max_epochs\\": 25,\\n",\n    "    \\"learning_rate\\": 1e-4,\\n",\n    "    \\"weight_decay\\": 1e-3,\\n",\n    "    \\"patience\\": 5,\\n",\n    "    \\"device\\": \\"cuda\\" if torch.cuda.is_available() else \\"cpu\\",\\n",\n    "\\n",\n    "    \\"small_kernel\\": 7,\\n",\n    "    \\"large_kernel\\": 49,\\n",\n    "    \\"n_filters\\": 64,\\n",\n    "    \\"dropout_p\\": 0.5,\\n",\n    "\\n",\n    "    \\"label_smoothing\\": \\"uniform\\",    # \\"none\\" | \\"uniform\\" | \\"structured\\"\\n",\n    "    \\"ls_alpha\\": 0.1,\\n",\n    "\\n",\n    "    \\"mc_dropout_samples\\": 30,\\n",\n    "    \\"balance_strategy\\": \\"oversample_and_flip\\",\\n",\n    "    \\"num_workers\\": 2,\\n",\n    "\\n",\n    "    \\"artifacts_dir\\": \\"artifacts\\",\\n",\n    "}\\n",\n    "os.makedirs(CONFIG[\\"artifacts_dir\\"], exist_ok=True)\\n",\n    "with open(os.path.join(CONFIG[\\"artifacts_dir\\"], \\"config.json\\"), \\"w\\") as f:\\n",\n    "    json.dump(CONFIG, f, indent=2)\\n",\n    "CONFIG"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "id": "a61f244e",\n   "metadata": {},\n   "source": [\n    "## 2. Dataset handling\\n",\n    "We target Sleep-EDF (Fpz-Cz @ 100 Hz). To keep the notebook runnable everywhere, we provide a **synthetic fallback** if EDF files are not present or cannot be downloaded. Replace the fallback with real parsing for your full results."\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "id": "df6c59bb",\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "import glob\\n",\n    "DATA_ROOT = os.path.join(CONFIG[\\"artifacts_dir\\"], \\"sleep_edf\\")\\n",\n    "os.makedirs(DATA_ROOT, exist_ok=True)\\n",\n    "\\n",\n    "STAGE_MAP = {\\"W\\":0,\\"N1\\":1,\\"N2\\":2,\\"N3\\":3,\\"R\\":4}\\n",\n    "IDX2STAGE = {v:k for k,v in STAGE_MAP.items()}\\n",\n    "\\n",\n    "def build_synthetic_dataset(n_subjects=4, epochs_per_subject=600, fs=100, epoch_sec=30):\\n",\n    "    rng = np.random.default_rng(CONFIG[\\"seed\\"])\\n",\n    "    X, y, subj = [], [], []\\n",\n    "    for s in range(n_subjects):\\n",\n    "        stages = rng.choice([0,1,2,3,4], size=epochs_per_subject, p=[0.2,0.1,0.45,0.15,0.1])\\n",\n    "        samples = fs*epoch_sec\\n",\n    "        signal = rng.normal(0, 1, size=(epochs_per_subject, samples))\\n",\n    "        X.append(signal); y.append(stages); subj += [f\\"SYN{s:02d}\\"]*epochs_per_subject\\n",\n    "    return np.vstack(X), np.hstack(y), np.array(subj)\\n",\n    "\\n",\n    "# For the template we use synthetic data so the rest of the pipeline is executable.\\n",\n    "X_epochs, y_epochs, subjects = build_synthetic_dataset()\\n",\n    "print(\\"Epochs:\\", X_epochs.shape, \\"Labels:\\", y_epochs.shape, \\"Subjects:\\", subjects.shape)\\n",\n    "\\n",\n    "def build_context_windows(Xe, ye, sub, context_epochs=3):\\n",\n    "    assert context_epochs == 3\\n",\n    "    Xw, yw, sw = [], [], []\\n",\n    "    for i in range(1, len(ye)-1):\\n",\n    "        win = np.concatenate([Xe[i-1], Xe[i], Xe[i+1]], axis=0)\\n",\n    "        Xw.append(win); yw.append(ye[i]); sw.append(sub[i])\\n",\n    "    Xw = np.vstack([np.array(x)[None,:] for x in Xw])\\n",\n    "    return Xw.astype(np.float32), np.array(yw).astype(np.int64), np.array(sw)\\n",\n    "\\n",\n    "X_win, y_win, subj_win = build_context_windows(X_epochs, y_epochs, subjects, CONFIG[\\"context_epochs\\"])\\n",\n    "\\n",\n    "def balance_data(X, y):\\n",\n    "    classes, counts = np.unique(y, return_counts=True)\\n",\n    "    max_c = counts.max()\\n",\n    "    Xb, yb = [], []\\n",\n    "    rng = np.random.default_rng(CONFIG[\\"seed\\"])\\n",\n    "    for c in classes:\\n",\n    "        idx = np.where(y == c)[0]\\n",\n    "        Xi, yi = X[idx], y[idx]\\n",\n    "        Xi_aug = np.concatenate([Xi, -Xi], axis=0)\\n",\n    "        yi_aug = np.concatenate([yi, yi], axis=0)\\n",\n    "        if len(Xi_aug) < max_c:\\n",\n    "            extra = rng.choice(len(Xi_aug), size=max_c-len(Xi_aug), replace=True)\\n",\n    "            Xi_aug = np.concatenate([Xi_aug, Xi_aug[extra]], axis=0)\\n",\n    "            yi_aug = np.concatenate([yi_aug, yi_aug[extra]], axis=0)\\n",\n    "        else:\\n",\n    "            Xi_aug = Xi_aug[:max_c]; yi_aug = yi_aug[:max_c]\\n",\n    "        Xb.append(Xi_aug); yb.append(yi_aug)\\n",\n    "    Xb = np.concatenate(Xb, axis=0); yb = np.concatenate(yb, axis=0)\\n",\n    "    perm = rng.permutation(len(yb))\\n",\n    "    return Xb[perm], yb[perm]\\n",\n    "\\n",\n    "if CONFIG[\\"balance_strategy\\"] == \\"oversample_and_flip\\":\\n",\n    "    X_bal, y_bal = balance_data(X_win, y_win)\\n",\n    "else:\\n",\n    "    X_bal, y_bal = X_win, y_win\\n",\n    "\\n",\n    "print(\\"Balanced windows:\\", X_bal.shape, \\"Labels:\\", y_bal.shape)"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "id": "510816c8",\n   "metadata": {},\n   "source": [\n    "## 3. Model: two-branch 1D CNN (DeepSleepNet-Lite style)\\n",\n    "Small kernels capture fine temporal features; large kernels capture broader spectral context. We keep parameter count modest and avoid RNNs."\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "id": "f130b6fb",\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "import torch.nn as nn\\n",\n    "\\n",\n    "class ConvBlock(nn.Module):\\n",\n    "    # Conv1d -> BN -> ReLU -> MaxPool\\n",\n    "    def __init__(self, in_ch, out_ch, k, pool=2):\\n",\n    "        super().__init__()\\n",\n    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, padding=k//2)\\n",\n    "        self.bn = nn.BatchNorm1d(out_ch, eps=1e-5, momentum=0.999)\\n",\n    "        self.act = nn.ReLU(inplace=True)\\n",\n    "        self.pool = nn.MaxPool1d(pool)\\n",\n    "    def forward(self, x):\\n",\n    "        return self.pool(self.act(self.bn(self.conv(x))))\\n",\n    "\\n",\n    "class DeepSleepNetLite(nn.Module):\\n",\n    "    # Input: [B, 1, T]; Output: class logits for 5 stages\\n",\n    "    def __init__(self, n_classes=5, nf=64, k_small=7, k_large=49, p_drop=0.5):\\n",\n    "        super().__init__()\\n",\n    "        self.s1 = ConvBlock(1, nf, k_small)\\n",\n    "        self.s2 = ConvBlock(nf, nf, k_small)\\n",\n    "        self.s3 = ConvBlock(nf, 2*nf, k_small)\\n",\n    "        self.s4 = ConvBlock(2*nf, 2*nf, k_small)\\n",\n    "\\n",\n    "        self.l1 = ConvBlock(1, nf, k_large)\\n",\n    "        self.l2 = ConvBlock(nf, nf, k_large)\\n",\n    "        self.l3 = ConvBlock(nf, 2*nf, k_large)\\n",\n    "        self.l4 = ConvBlock(2*nf, 2*nf, k_large)\\n",\n    "\\n",\n    "        self.drop = nn.Dropout(p_drop)\\n",\n    "        self.head = nn.Sequential(\\n",\n    "            nn.AdaptiveAvgPool1d(1),\\n",\n    "            nn.Flatten(),\\n",\n    "            nn.Linear(4*nf, n_classes)\\n",\n    "        )\\n",\n    "\\n",\n    "    def forward(self, x):\\n",\n    "        xs = self.s4(self.s3(self.s2(self.s1(x))))\\n",\n    "        xl = self.l4(self.l3(self.l2(self.l1(x))))\\n",\n    "        xcat = torch.cat([xs, xl], dim=1)\\n",\n    "        xcat = self.drop(xcat)\\n",\n    "        return self.head(xcat)\\n",\n    "\\n",\n    "# Simple shape check\\n",\n    "tmp = torch.randn(2,1,CONFIG[\\"fs\\"]*CONFIG[\\"epoch_seconds\\"]*CONFIG[\\"context_epochs\\"])\\n",\n    "print(\\"Logits shape:\\", DeepSleepNetLite().forward(tmp).shape)"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "id": "cfaee213",\n   "metadata": {},\n   "source": [\n    "## 4. Label smoothing\\n",\n    "Uniform (LSu) mixes one-hot with uniform; structured (LSs) mixes one-hot with an empirical transition-based distribution M(prev,:,next) estimated from training labels."\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "id": "388884fa",\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "def make_uniform_targets(y, n_classes=5, alpha=0.1, device=\\"cpu\\"):\\n",\n    "    off = alpha / n_classes\\n",\n    "    on = 1.0 - alpha + off\\n",\n    "    tgt = torch.full((len(y), n_classes), off, dtype=torch.float32, device=device)\\n",\n    "    tgt[torch.arange(len(y)), y] = on\\n",\n    "    return tgt\\n",\n    "\\n",\n    "def estimate_M_structured(y_sequence, n_classes=5):\\n",\n    "    M = np.ones((n_classes, n_classes, n_classes), dtype=np.float64)  # Laplace smoothing\\n",\n    "    for i in range(1, len(y_sequence)-1):\\n",\n    "        p, c, n = y_sequence[i-1], y_sequence[i], y_sequence[i+1]\\n",\n    "        M[p, c, n] += 1.0\\n",\n    "    # normalize over middle dimension\\n",\n    "    M = M / M.sum(axis=1, keepdims=True)\\n",\n    "    M = M / M.sum(axis=2, keepdims=True)\\n",\n    "    return M\\n",\n    "\\n",\n    "def make_structured_targets(y, y_prev, y_next, M, alpha=0.2, n_classes=5, device=\\"cpu\\"):\\n",\n    "    B = len(y)\\n",\n    "    tgt = torch.zeros((B, n_classes), dtype=torch.float32, device=device)\\n",\n    "    for i in range(B):\\n",\n    "        vec = torch.tensor(M[y_prev[i].item(), :, y_next[i].item()], dtype=torch.float32, device=device)\\n",\n    "        vec = vec / (vec.sum() + 1e-8)\\n",\n    "        tgt[i] = (1.0 - alpha) * F.one_hot(y[i], n_classes).float() + alpha * vec\\n",\n    "    return tgt"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "id": "03d56ef3",\n   "metadata": {},\n   "source": [\n    "## 5. Dataset wrapper and CV splits (by subject)\\n",\n    "We wrap arrays into a Dataset that also carries prev/next labels for LSs, and create subject-wise folds."\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "id": "3027afbd",\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "from collections import defaultdict\\n",\n    "\\n",\n    "class SleepWindows(Dataset):\\n",\n    "    def __init__(self, X, y, subj, prev_labels, next_labels):\\n",\n    "        self.X = X.astype(np.float32)\\n",\n    "        self.y = y.astype(np.int64)\\n",\n    "        self.subj = np.array(subj)\\n",\n    "        self.prev = prev_labels.astype(np.int64)\\n",\n    "        self.next = next_labels.astype(np.int64)\\n",\n    "    def __len__(self): return len(self.y)\\n",\n    "    def __getitem__(self, idx):\\n",\n    "        x = self.X[idx][None, :]  # [1, T]\\n",\n    "        return x, self.y[idx], self.prev[idx], self.next[idx]\\n",\n    "\\n",\n    "prev_arr = np.concatenate([[y_bal[0]], y_bal[:-1]])\\n",\n    "next_arr = np.concatenate([y_bal[1:], [y_bal[-1]]])\\n",\n    "dataset = SleepWindows(X_bal, y_bal, subj_win[:len(y_bal)], prev_arr, next_arr)\\n",\n    "\\n",\n    "uniq_subj = sorted(list(set(dataset.subj)))\\n",\n    "rng = np.random.default_rng(CONFIG[\\"seed\\"])\\n",\n    "rng.shuffle(uniq_subj)\\n",\n    "folds = np.array_split(uniq_subj, CONFIG[\\"cv_folds\\"])\\n",\n    "print(\\"Folds (subjects):\\", [list(f) for f in folds])"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "id": "676e2588",\n   "metadata": {},\n   "source": [\n    "## 6. Utilities: Early stopping, ECE, reliability plot"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "id": "f418de1b",\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "def ece_score(confidences, correctness, n_bins=15):\\n",\n    "    bins = np.linspace(0., 1., n_bins+1)\\n",\n    "    ece = 0.0\\n",\n    "    for i in range(n_bins):\\n",\n    "        lo, hi = bins[i], bins[i+1]\\n",\n    "        sel = (confidences > lo) & (confidences <= hi)\\n",\n    "        if sel.sum() == 0: continue\\n",\n    "        acc = correctness[sel].mean()\\n",\n    "        conf = confidences[sel].mean()\\n",\n    "        ece += (sel.sum() / len(confidences)) * abs(acc - conf)\\n",\n    "    return float(ece)\\n",\n    "\\n",\n    "def plot_reliability(confidences, correctness, n_bins=15):\\n",\n    "    bins = np.linspace(0., 1., n_bins+1)\\n",\n    "    xs, ys = [], []\\n",\n    "    for i in range(n_bins):\\n",\n    "        lo, hi = bins[i], bins[i+1]\\n",\n    "        sel = (confidences > lo) & (confidences <= hi)\\n",\n    "        if sel.sum() == 0: continue\\n",\n    "        xs.append(correctness[sel].mean()); ys.append(confidences[sel].mean())\\n",\n    "    plt.figure()\\n",\n    "    plt.plot([0,1],[0,1],\'--\', linewidth=1)\\n",\n    "    plt.scatter(xs, ys)\\n",\n    "    plt.xlabel(\\"Accuracy per bin\\"); plt.ylabel(\\"Mean confidence per bin\\"); plt.title(\\"Reliability Diagram\\")\\n",\n    "\\n",\n    "class EarlyStopper:\\n",\n    "    def __init__(self, patience=5):\\n",\n    "        self.best = None; self.wait = 0; self.pat = patience; self.state=None\\n",\n    "    def step(self, metric, model):\\n",\n    "        if self.best is None or metric > self.best:\\n",\n    "            self.best = metric; self.wait = 0\\n",\n    "            self.state = {k:v.cpu().clone() for k,v in model.state_dict().items()}\\n",\n    "            return False\\n",\n    "        self.wait += 1\\n",\n    "        return self.wait > self.pat"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "id": "930ee73c",\n   "metadata": {},\n   "source": [\n    "## 7. Train per fold\\n",\n    "We train with the selected label smoothing strategy and early stopping on macro-F1 (more robust to class imbalance)."\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "id": "e2fd0b79",\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "def indices_for_fold(fold_id):\\n",\n    "    test_subj = set(folds[fold_id])\\n",\n    "    train_subj = set().union(*[set(f) for i,f in enumerate(folds) if i!=fold_id])\\n",\n    "    train_mask = np.array([s in train_subj for s in dataset.subj])\\n",\n    "    test_mask  = np.array([s in test_subj  for s in dataset.subj])\\n",\n    "    idx_train_all = np.where(train_mask)[0]\\n",\n    "    idx_test = np.where(test_mask)[0]\\n",\n    "    rng = np.random.default_rng(CONFIG[\\"seed\\"] + fold_id)\\n",\n    "    perm = rng.permutation(len(idx_train_all))\\n",\n    "    n_val = max(1, int(CONFIG[\\"val_ratio\\"] * len(idx_train_all)))\\n",\n    "    idx_val = idx_train_all[perm[:n_val]]\\n",\n    "    idx_train = idx_train_all[perm[n_val:]]\\n",\n    "    return idx_train, idx_val, idx_test\\n",\n    "\\n",\n    "def run_fold(train_idx, val_idx, test_idx, data):\\n",\n    "    set_seeds(CONFIG[\\"seed\\"])\\n",\n    "    device = CONFIG[\\"device\\"]\\n",\n    "    model = DeepSleepNetLite(nf=CONFIG[\\"n_filters\\"],\\n",\n    "                             k_small=CONFIG[\\"small_kernel\\"],\\n",\n    "                             k_large=CONFIG[\\"large_kernel\\"],\\n",\n    "                             p_drop=CONFIG[\\"dropout_p\\"]).to(device)\\n",\n    "    opt = optim.Adam(model.parameters(), lr=CONFIG[\\"learning_rate\\"], weight_decay=CONFIG[\\"weight_decay\\"])\\n",\n    "\\n",\n    "    def make_loader(idxs, shuffle):\\n",\n    "        subset = torch.utils.data.Subset(data, idxs.tolist())\\n",\n    "        return DataLoader(subset, batch_size=CONFIG[\\"batch_size\\"], shuffle=shuffle,\\n",\n    "                          num_workers=CONFIG[\\"num_workers\\"], drop_last=False)\\n",\n    "\\n",\n    "    train_loader = make_loader(train_idx, True)\\n",\n    "    val_loader   = make_loader(val_idx, False)\\n",\n    "\\n",\n    "    M = estimate_M_structured(data.y[train_idx]) if CONFIG[\\"label_smoothing\\"] == \\"structured\\" else None\\n",\n    "    stopper = EarlyStopper(CONFIG[\\"patience\\"])\\n",\n    "    history = []\\n",\n    "    best_epoch = -1\\n",\n    "\\n",\n    "    for epoch in range(1, CONFIG[\\"max_epochs\\"]+1):\\n",\n    "        model.train()\\n",\n    "        losses = []\\n",\n    "        for xb, yb, pb, nb in train_loader:\\n",\n    "            xb, yb, pb, nb = xb.to(device), yb.to(device), pb.to(device), nb.to(device)\\n",\n    "            logits = model(xb)\\n",\n    "            if CONFIG[\\"label_smoothing\\"] == \\"uniform\\":\\n",\n    "                targets = make_uniform_targets(yb, n_classes=5, alpha=CONFIG[\\"ls_alpha\\"], device=device)\\n",\n    "                loss = -(targets * F.log_softmax(logits, dim=1)).sum(dim=1).mean()\\n",\n    "            elif CONFIG[\\"label_smoothing\\"] == \\"structured\\":\\n",\n    "                targets = make_structured_targets(yb, pb, nb, M, alpha=CONFIG[\\"ls_alpha\\"], device=device)\\n",\n    "                loss = -(targets * F.log_softmax(logits, dim=1)).sum(dim=1).mean()\\n",\n    "            else:\\n",\n    "                loss = F.cross_entropy(logits, yb)\\n",\n    "            opt.zero_grad(); loss.backward(); opt.step()\\n",\n    "            losses.append(loss.item())\\n",\n    "\\n",\n    "        model.eval()\\n",\n    "        y_true, y_pred, y_conf = [], [], []\\n",\n    "        with torch.no_grad():\\n",\n    "            for xb, yb, _, _ in val_loader:\\n",\n    "                xb, yb = xb.to(device), yb.to(device)\\n",\n    "                probs = F.softmax(model(xb), dim=1)\\n",\n    "                conf, pred = probs.max(dim=1)\\n",\n    "                y_true.extend(yb.cpu().numpy()); y_pred.extend(pred.cpu().numpy()); y_conf.extend(conf.cpu().numpy())\\n",\n    "        acc = accuracy_score(y_true, y_pred)\\n",\n    "        mf1 = f1_score(y_true, y_pred, average=\\"macro\\")\\n",\n    "        wf1 = f1_score(y_true, y_pred, average=\\"weighted\\")\\n",\n    "        kappa = cohen_kappa_score(y_true, y_pred)\\n",\n    "        ece = ece_score(np.array(y_conf), (np.array(y_true)==np.array(y_pred)).astype(float))\\n",\n    "        history.append({\\"epoch\\":epoch,\\"loss\\":float(np.mean(losses)),\\"acc\\":acc,\\"mf1\\":mf1,\\"wf1\\":wf1,\\"kappa\\":kappa,\\"ece\\":ece})\\n",\n    "        print(f\\"Epoch {epoch:02d} | loss {np.mean(losses):.4f} | acc {acc:.3f} | MF1 {mf1:.3f} | k {kappa:.3f} | ECE {ece:.3f}\\")\\n",\n    "        if stopper.step(mf1, model):\\n",\n    "            print(\\"Early stopping\\"); break\\n",\n    "        else:\\n",\n    "            best_epoch = epoch\\n",\n    "\\n",\n    "    model.load_state_dict(stopper.state)\\n",\n    "    return model, pd.DataFrame(history), best_epoch"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "id": "4262afdf",\n   "metadata": {},\n   "source": [\n    "## 8. Run K-fold CV, save models and histories"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "id": "c3d8361a",\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "ALL = []\\n",\n    "FOLD_RESULTS = []\\n",\n    "for fid in range(len(folds)):\\n",\n    "    print(\\"\\\\\\\\n=== Fold\\", fid, \\"===\\")\\n",\n    "    tr, va, te = indices_for_fold(fid)\\n",\n    "    model, hist, best_ep = run_fold(tr, va, te, dataset)\\n",\n    "    save_path = os.path.join(CONFIG[\\"artifacts_dir\\"], f\\"model_fold{fid}.pt\\")\\n",\n    "    torch.save(model.state_dict(), save_path)\\n",\n    "    hist.to_csv(os.path.join(CONFIG[\\"artifacts_dir\\"], f\\"history_fold{fid}.csv\\"), index=False)\\n",\n    "    FOLD_RESULTS.append({\\"fold\\": fid, \\"best_epoch\\": int(best_ep), \\"model_path\\": save_path})\\n",\n    "pd.DataFrame(FOLD_RESULTS)"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "id": "389a6146",\n   "metadata": {},\n   "source": [\n    "## 9. Test evaluation + MC Dropout + Selective prediction"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "id": "5ba9720c",\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "def enable_mc_dropout(m):\\n",\n    "    for module in m.modules():\\n",\n    "        if isinstance(module, nn.Dropout):\\n",\n    "            module.train()\\n",\n    "\\n",\n    "def evaluate_model(model, idxs, mc_samples=0):\\n",\n    "    device = CONFIG[\\"device\\"]\\n",\n    "    loader = DataLoader(torch.utils.data.Subset(dataset, idxs.tolist()),\\n",\n    "                        batch_size=CONFIG[\\"batch_size\\"], shuffle=False,\\n",\n    "                        num_workers=CONFIG[\\"num_workers\\"], drop_last=False)\\n",\n    "    y_true, y_pred, y_conf = [], [], []\\n",\n    "    if mc_samples <= 0:\\n",\n    "        model.eval()\\n",\n    "        with torch.no_grad():\\n",\n    "            for xb, yb, _, _ in loader:\\n",\n    "                xb, yb = xb.to(device), yb.to(device)\\n",\n    "                probs = F.softmax(model(xb), dim=1)\\n",\n    "                conf, pred = probs.max(dim=1)\\n",\n    "                y_true.extend(yb.cpu().numpy()); y_pred.extend(pred.cpu().numpy()); y_conf.extend(conf.cpu().numpy())\\n",\n    "        return np.array(y_true), np.array(y_pred), np.array(y_conf)\\n",\n    "    else:\\n",\n    "        model.eval(); enable_mc_dropout(model)\\n",\n    "        with torch.no_grad():\\n",\n    "            for xb, yb, _, _ in loader:\\n",\n    "                xb, yb = xb.to(device), yb.to(device)\\n",\n    "                probs_mc = []\\n",\n    "                for _ in range(mc_samples):\\n",\n    "                    probs_mc.append(F.softmax(model(xb), dim=1).unsqueeze(0))\\n",\n    "                probs_mc = torch.cat(probs_mc, dim=0)\\n",\n    "                probs = probs_mc.mean(dim=0)\\n",\n    "                conf, pred = probs.max(dim=1)\\n",\n    "                y_true.extend(yb.cpu().numpy()); y_pred.extend(pred.cpu().numpy()); y_conf.extend(conf.cpu().numpy())\\n",\n    "        return np.array(y_true), np.array(y_pred), np.array(y_conf)\\n",\n    "\\n",\n    "ALL_METRICS = []\\n",\n    "for rec in FOLD_RESULTS:\\n",\n    "    fid = rec[\\"fold\\"]\\n",\n    "    _, _, idx_test = indices_for_fold(fid)\\n",\n    "\\n",\n    "    model = DeepSleepNetLite(nf=CONFIG[\\"n_filters\\"],\\n",\n    "                             k_small=CONFIG[\\"small_kernel\\"],\\n",\n    "                             k_large=CONFIG[\\"large_kernel\\"],\\n",\n    "                             p_drop=CONFIG[\\"dropout_p\\"]).to(CONFIG[\\"device\\"])\\n",\n    "    model.load_state_dict(torch.load(rec[\\"model_path\\"], map_location=CONFIG[\\"device\\"]))\\n",\n    "\\n",\n    "    y_t, y_p, y_c = evaluate_model(model, idx_test, mc_samples=0)\\n",\n    "    acc = accuracy_score(y_t, y_p); mf1 = f1_score(y_t, y_p, average=\\"macro\\")\\n",\n    "    wf1 = f1_score(y_t, y_p, average=\\"weighted\\"); kappa = cohen_kappa_score(y_t, y_p)\\n",\n    "    ece = ece_score(y_c, (y_t==y_p).astype(float))\\n",\n    "    print(f\\"[Fold {fid}] TEST std: acc {acc:.3f} mf1 {mf1:.3f} k {kappa:.3f} ece {ece:.3f}\\")\\n",\n    "    ALL_METRICS.append({\\"fold\\":fid,\\"mode\\":\\"standard\\",\\"acc\\":acc,\\"mf1\\":mf1,\\"wf1\\":wf1,\\"kappa\\":kappa,\\"ece\\":ece})\\n",\n    "    plot_reliability(y_c, (y_t==y_p).astype(float))\\n",\n    "\\n",\n    "    S = CONFIG[\\"mc_dropout_samples\\"]\\n",\n    "    y_t2, y_p2, y_c2 = evaluate_model(model, idx_test, mc_samples=S)\\n",\n    "    acc2 = accuracy_score(y_t2, y_p2); mf1_2 = f1_score(y_t2, y_p2, average=\\"macro\\")\\n",\n    "    wf1_2 = f1_score(y_t2, y_p2, average=\\"weighted\\"); kappa2 = cohen_kappa_score(y_t2, y_p2)\\n",\n    "    ece2 = ece_score(y_c2, (y_t2==y_p2).astype(float))\\n",\n    "    print(f\\"[Fold {fid}] TEST MC{S}: acc {acc2:.3f} mf1 {mf1_2:.3f} k {kappa2:.3f} ece {ece2:.3f}\\")\\n",\n    "    ALL_METRICS.append({\\"fold\\":fid,\\"mode\\":f\\"mc{S}\\",\\"acc\\":acc2,\\"mf1\\":mf1_2,\\"wf1\\":wf1_2,\\"kappa\\":kappa2,\\"ece\\":ece2})\\n",\n    "\\n",\n    "    for q in [0.05, 0.1, 0.2]:\\n",\n    "        thr = np.quantile(y_c2, q)\\n",\n    "        mask = y_c2 > thr\\n",\n    "        if mask.sum()==0: continue\\n",\n    "        acc_q = accuracy_score(y_t2[mask], y_p2[mask])\\n",\n    "        mf1_q = f1_score(y_t2[mask], y_p2[mask], average=\\"macro\\")\\n",\n    "        kappa_q = cohen_kappa_score(y_t2[mask], y_p2[mask])\\n",\n    "        print(f\\"[Fold {fid}] MC{S} reject {int(q*100)}%: acc {acc_q:.3f} mf1 {mf1_q:.3f} k {kappa_q:.3f}\\")\\n",\n    "        ALL_METRICS.append({\\"fold\\":fid,\\"mode\\":f\\"mc{S}_reject_{int(q*100)}\\",\\"acc\\":acc_q,\\"mf1\\":mf1_q,\\"wf1\\":None,\\"kappa\\":kappa_q,\\"ece\\":None})\\n",\n    "\\n",\n    "pd.DataFrame(ALL_METRICS)"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "id": "f0d62911",\n   "metadata": {},\n   "source": [\n    "## 10. Qualitative inspection\\n",\n    "We show a couple of sample windows with predicted distribution (mean if MC) to understand errors and uncertainty."\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "id": "1c56d9c9",\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "def show_examples(model, idxs, n=2, mc_samples=0):\\n",\n    "    device = CONFIG[\\"device\\"]\\n",\n    "    loader = DataLoader(torch.utils.data.Subset(dataset, idxs.tolist()[:n]), batch_size=1, shuffle=False)\\n",\n    "    model.eval()\\n",\n    "    if mc_samples>0:\\n",\n    "        for m in model.modules():\\n",\n    "            if isinstance(m, nn.Dropout): m.train()\\n",\n    "    with torch.no_grad():\\n",\n    "        for xb, yb, _, _ in loader:\\n",\n    "            xb = xb.to(device); gt = int(yb.item())\\n",\n    "            if mc_samples>0:\\n",\n    "                pm = []\\n",\n    "                for _ in range(mc_samples):\\n",\n    "                    pm.append(F.softmax(model(xb), dim=1).cpu().numpy()[0])\\n",\n    "                probs = np.mean(pm, axis=0)\\n",\n    "            else:\\n",\n    "                probs = F.softmax(model(xb), dim=1).cpu().numpy()[0]\\n",\n    "            pred = int(np.argmax(probs))\\n",\n    "            plt.figure(); plt.plot(xb.cpu().numpy().ravel()); plt.title(f\\"True {IDX2STAGE[gt]} | Pred {IDX2STAGE[pred]} | Conf {probs[pred]:.2f}\\")\\n",\n    "            plt.figure(); plt.bar(range(5), probs); plt.xticks(range(5), [IDX2STAGE[i] for i in range(5)]); plt.title(\\"Predicted probabilities\\")\\n",\n    "\\n",\n    "_, _, idx_test0 = indices_for_fold(0)\\n",\n    "m0 = DeepSleepNetLite(nf=CONFIG[\\"n_filters\\"], k_small=CONFIG[\\"small_kernel\\"], k_large=CONFIG[\\"large_kernel\\"], p_drop=CONFIG[\\"dropout_p\\"]).to(CONFIG[\\"device\\"])\\n",\n    "m0.load_state_dict(torch.load(os.path.join(CONFIG[\\"artifacts_dir\\"], \\"model_fold0.pt\\"), map_location=CONFIG[\\"device\\"]))\\n",\n    "show_examples(m0, idx_test0, n=2, mc_samples=CONFIG[\\"mc_dropout_samples\\"])"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "id": "e8e0e295",\n   "metadata": {},\n   "source": [\n    "## 11. Export artifacts for final submission\\n",\n    "We save metrics, a minimal `model.txt`, `data.txt`, and a `requirements.txt` as requested by the brief."\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "id": "186a4365",\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "metrics_path = os.path.join(CONFIG[\\"artifacts_dir\\"], \\"metrics.csv\\")\\n",\n    "pd.DataFrame(ALL_METRICS).to_csv(metrics_path, index=False)\\n",\n    "print(\\"Saved:\\", metrics_path)\\n",\n    "\\n",\n    "with open(os.path.join(CONFIG[\\"artifacts_dir\\"], \\"model.txt\\"), \\"w\\") as f:\\n",\n    "    f.write(\\"# Saved model weights (per fold)\\\\n\\")\\n",\n    "    for rec in os.listdir(CONFIG[\\"artifacts_dir\\"]):\\n",\n    "        if rec.startswith(\\"model_fold\\") and rec.endswith(\\".pt\\"):\\n",\n    "            f.write(rec + \\"\\\\n\\")\\n",\n    "\\n",\n    "with open(os.path.join(CONFIG[\\"artifacts_dir\\"], \\"data.txt\\"), \\"w\\") as f:\\n",\n    "    f.write(\\"Sleep-EDF Expanded (Sleep Cassette). PhysioNet.\\\\nPlace EDF files under artifacts/sleep_edf/ if auto-download is not available.\\\\n\\")\\n",\n    "\\n",\n    "reqs = [\\n",\n    "    \\"numpy\\",\\"scipy\\",\\"pandas\\",\\"matplotlib\\",\\"scikit-learn\\",\\n",\n    "    \\"torch==2.2.2\\",\\"torchmetrics>=1.3.0\\",\\"tqdm\\",\\"wfdb\\",\\"mne\\"\\n",\n    "]\\n",\n    "with open(os.path.join(CONFIG[\\"artifacts_dir\\"], \\"requirements.txt\\"), \\"w\\") as f:\\n",\n    "    f.write(\\"\\\\\\\\n\\".join(reqs))\\n",\n    "\\n",\n    "print(\\"Artifacts in:\\", CONFIG[\\"artifacts_dir\\"])"\n   ]\n  }\n ],\n "metadata": {},\n "nbformat": 4,\n "nbformat_minor": 5\n}\n'
    nb = nbformat.reads(NOTEBOOK_FULL, as_version=4)

    out_path = os.path.join(os.getcwd(), "DeepSleepNetLite_Project_Notebook.ipynb")
    with open(out_path, "w", encoding="utf-8") as f:
        nbformat.write(nb, f)
    print("Wrote", out_path)

if __name__ == "__main__":
    # For simplicity we just write out the baked notebook content.
    build()
